{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random_normal(shape=[16,4], dtype=tf.float32)\n",
    "b = tf.one_hot(tf.constant([0,0,0,0,1,1,1,2,2,2,3,3,3,2,1,3]),depth=4)\n",
    "c = tf.reduce_sum(a * b, axis = -1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x,y,z = sess.run([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.50538075  0.44697276 -0.14002024 -2.03396487]\n",
      " [ 0.05067395 -0.45982233 -0.32494169 -0.12284373]\n",
      " [ 1.41731477 -0.9119519   1.51394963 -0.65013605]\n",
      " [ 0.89202952  0.62645262  0.52674538 -0.6262579 ]\n",
      " [ 0.65703481  0.63458788 -0.4508917  -0.24278168]\n",
      " [ 0.79539788 -0.87796795  0.37232098  0.8916586 ]\n",
      " [-0.24759692 -1.4219656   0.44021103 -0.90602392]\n",
      " [-0.40116554  1.2351898  -1.01278329  0.08814283]\n",
      " [ 1.27070558  1.33237183 -0.3653762  -1.50765073]\n",
      " [-0.16447587  1.90410519  1.51378405 -0.8397491 ]\n",
      " [ 0.64948243  0.33649352  2.17079663 -0.45793384]\n",
      " [-0.32103696  0.09110694  1.04220772 -1.27053106]\n",
      " [-0.16583857 -0.7216785   0.28754005 -0.11295979]\n",
      " [ 1.3425082  -1.56153238 -2.30320191 -0.55602211]\n",
      " [-0.48113894 -1.23819602 -0.13900961  1.16311073]\n",
      " [ 1.25212169  1.15608799 -0.85099274  0.41163331]]\n",
      "[-1.50538075  0.05067395  1.41731477  0.89202952  0.63458788 -0.87796795\n",
      " -1.4219656  -1.01278329 -0.3653762   1.51378405 -0.45793384 -1.27053106\n",
      " -0.11295979 -2.30320191 -1.23819602  0.41163331]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork():\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    def __init__(self, state_size, action_size, optimizer, gamma = 0.9, seed = 42):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, example: (8,)\n",
    "            action_size (int): Dimension of each action, example: 4\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.optim = optimizer\n",
    "        self.gamma = gamma\n",
    "\n",
    "        print(\"State size: %i\" % self.state_size)\n",
    "        print(\"Action size: %i\" % self.action_size)\n",
    "        # Initalize\n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(seed)\n",
    "        self.sess = tf.Session() # Prepare a tensorflow sesion\n",
    "\n",
    "        # Prepared placeholders\n",
    "        self.state = tf.placeholder(shape = [None, self.state_size], dtype = tf.float32) # input: S\n",
    "        self.next_state = tf.placeholder(shape = [None, self.state_size], dtype = tf.float32) # input: S'\n",
    "        self.action = tf.placeholder(shape = [None, 1], dtype = tf.int32) # action\n",
    "        self.reward = tf.placeholder(shape = [None, 1], dtype = tf.float32) # reward\n",
    "        self.done = tf.placeholder(shape = [None, 1], dtype = tf.float32) # done or not\n",
    "\n",
    "        self.action = tf.one_hot(self.action, depth = action_size)\n",
    "\n",
    "        # Build networks\n",
    "        with tf.variable_scope(\"Qtable\"):\n",
    "            neurons_of_layers = [64, 64, 64]\n",
    "            # Use to update/train the agent's brain\n",
    "            # Used to get Q(s, a)\n",
    "            self.q_local = self._build_model(x = self.state, \n",
    "                                             neurons_of_layers = neurons_of_layers, \n",
    "                                             scope = 'local', trainable = True)\n",
    "\n",
    "            # with fixed parameters, used to get Q(s', a)\n",
    "            self.q_target = self._build_model(x = self.next_state, \n",
    "                                              neurons_of_layers = neurons_of_layers, \n",
    "                                              scope = 'target', trainable = False)\n",
    "        \n",
    "        # Handlers of parameters\n",
    "        self.localnet_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'Qtable/local')\n",
    "        self.targetnet_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'Qtable/target')\n",
    "        self.params_replace = [tf.assign(old, new) for old, new in zip(self.localnet_params, self.targetnet_params)]\n",
    "\n",
    "        # Compute loss (TD-loss), TD_error = lr * ((reward + gamma * max(Q(s',A)) - Q(s,a))\n",
    "        td_target = self.reward + self.gamma * tf.reduce_max(self.q_target, axis = -1) * (1. - self.done)\n",
    "        td_expect = tf.reduce_sum(self.q_local*self.action, axis = -1)\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(td_target, td_expect))\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.update_ops = self.optim.minimize(self.loss, var_list = self.localnet_params)\n",
    "\n",
    "        # Finally, initalize weights\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        print(\"Network Ready\")\n",
    "\n",
    "    def _build_model(self, x, scope, trainable, neurons_of_layers = [25,25,25], acti = tf.nn.relu):\n",
    "        \"\"\"\n",
    "        x: input_placeholder\n",
    "        scope: local network or target network\n",
    "        trainable: control if it is controlable or not\n",
    "        neurons_of_layers: numbers of units for each layers\n",
    "        acti: activation\n",
    "        \"\"\"\n",
    "        # Defined block function, useful when repeats\n",
    "        def mlp_block(x, name, units, activation, trainable = True):\n",
    "            with tf.variable_scope(name):\n",
    "                x = tf.layers.dense(x, units = units, trainable = trainable)\n",
    "                x = activation(x)\n",
    "            return x\n",
    "            \n",
    "        # Build a simple multi-layers network\n",
    "        with tf.variable_scope(scope):\n",
    "            for i, n_unit in enumerate(neurons_of_layers):\n",
    "                block_name = 'block_' + str(i)\n",
    "                # if i = 0 (first block), inputs should be input_placeholder\n",
    "                x = mlp_block(x = x, \n",
    "                    name = block_name, \n",
    "                    units = n_unit, \n",
    "                    activation = acti, \n",
    "                    trainable = trainable)\n",
    "            x = tf.layers.dense(x, units = self.action_size, trainable = trainable)\n",
    "        return x\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Generate action from localnet\n",
    "        get the action with the highest state-action pair\n",
    "        \"\"\"\n",
    "        action = np.argmax(self.sess.run(self.q_local, feed_dict = {self.state:state}))\n",
    "        return action\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"\n",
    "        Train the localnet\n",
    "        - Args:\n",
    "            - batch: (state, action, reward, next_state, done)\n",
    "        - Returns:\n",
    "            - loss\n",
    "        \"\"\"\n",
    "        current_loss, _ = self.sess.run([self.loss, self.update_ops], feed_dict= {self.state:state,\n",
    "                                                                                  self.action: action,\n",
    "                                                                                  self.reward: reward,\n",
    "                                                                                  self.next_state: next_state,\n",
    "                                                                                  self.done: done})\n",
    "        return current_loss\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Swap memory from localnet to targetnet,\n",
    "        simply session run the replacement ops\n",
    "        \"\"\"\n",
    "        self.sess.run(self.params_replace)\n",
    "\n",
    "    def save_model(self, model_name = None):\n",
    "        \"\"\"\n",
    "        Save the model (save localnet, we don't save target net)\n",
    "        \"\"\"\n",
    "        saver.save(self.sess, 'dqn.ckpt' if model_name is None else model_name)\n",
    "        print(\"model saved\")\n",
    "\n",
    "    def load_model(self, model_name = None):\n",
    "        \"\"\"\n",
    "        Load the model\n",
    "        \"\"\"\n",
    "        saver.restore(self.sess, 'dqn.ckpt' if model_name is None else model_name)\n",
    "        print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None])\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import sys\n",
    "\n",
    "#from model import QNetwork\n",
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate= LR)\n",
    "        self.Qnetwork = QNetwork(state_size = state_size, \n",
    "                                 action_size = action_size, \n",
    "                                 optimizer = optimizer,\n",
    "                                 gamma=GAMMA)\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            action_value = self.Qnetwork.get_action(state)\n",
    "            return np.argmax(action_values)\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        #states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## TODO: compute and minimize the loss\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        current_loss = self.Qnetwork.train(experiences)\n",
    "        print('\\rCurrent loss: %.3f' % current_loss)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.Qnetwork.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
